Local LLM Web App — High-Level Architecture

1. Overview
The goal is to create a Streamlit-based web UI that manages a conversation between two locally hosted models (served via ollama).

2. Tech Stack
• Frontend/UI: Streamlit (Python)
• Local Models: local Ollama server
• Backend Flow: Simple REST calls to/from the model endpoints

3. Architecture
  a) Model Endpoints
   - Each LLM runs from a local server.
   - Accepts a prompt in JSON.
   - Returns generated text in JSON.
  b) Streamlit App
   - Maintains conversation in `st.session_state`.
   - For each turn, sends the previous response to the other model.
   - Displays responses in a chat-like interface.

4. Conversation Flow
   - User Input: The user provides an initial prompt in Streamlit.
   - Model A: Streamlit sends the prompt to Model A’s endpoint, gets the response.
   - Model B: The response becomes Model B’s prompt. Model B returns its output.
   - Loop: Continue alternating until the conversation reaches its limit.

5. Deployment
   • App is showcased to users via Cloudflare web tunnels.
   • Run a shell script that starts a quick tunnel and the local Streamlit application.
   • Access: Open the URL generated by Cloudflare.

6. Next Steps
   • Refine prompt formatting.
   • Handle potential errors and timeouts.
   • Optionally add features like streaming responses or advanced prompt engineering.
