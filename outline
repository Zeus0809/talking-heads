Local LLM Web App — High-Level Architecture

1. Overview
The goal is to create a Streamlit-based web UI that manages a conversation between two locally hosted models (served via HTTP endpoints generated by LM Studio).

2. Tech Stack
• Frontend/UI: Streamlit (Python)
• Local Models: LM Studio (hosting models via HTTP endpoints)
• Backend Flow: Simple REST calls to/from the model endpoints

3. Architecture
  a) Model Endpoints
   - Each LLM runs as a local server.
   - Accepts a prompt in JSON.
   - Returns generated text in JSON.
  b) Streamlit App
   - Maintains conversation in `st.session_state`.
   - For each turn, sends the previous response to the other model.
   - Displays responses in a chat-like interface.

4. Conversation Flow
   - User Input: The user provides an initial prompt in Streamlit.
   - Model A: Streamlit sends the prompt to Model A’s endpoint, gets the response.
   - Model B: The response becomes Model B’s prompt. Model B returns its output.
   - Loop: Continue alternating until the user stops.

5. Deployment
   • Local: Start each model’s HTTP server.
   • Run Streamlit: `streamlit run app.py`.
   • Access: Open the local URL to use the app.

6. Next Steps
   • Refine prompt formatting.
   • Handle potential errors and timeouts.
   • Optionally add features like streaming responses or advanced prompt engineering.
