# ollama llm response WITHOUT streaming
ChatResponse ( model='phi3:3.8b',
               created_at='2025-03-28T18:09:27.113075Z',
               done=True, done_reason='stop',
               total_duration=6128319750,
               load_duration=551142541,
               prompt_eval_count=54,
               prompt_eval_duration=3487153250,
               eval_count=68,
               eval_duration=2088392625,
               message = Message ( role='assistant',
                                 content="Absolutely adore it! Tea is life's little luxury that brings joy and relaxation. Recommended daily ritual—it’s essential for a balanced mind. Enjoy the art of tea steeping to its fullest potential, folks! Cheers to tranquility in every cup!",
                                 images=None,
                                 tool_calls=None ) )

# ollama llm response WITH streaming
<generator object Client._request.<locals>.inner at 0x126cb7d00>

# Streamed response returns a series of json objects like this, where each object has one token of message text:
    { model='phi3:3.8b'
    created_at='2025-03-28T18:29:27.823486Z'
    done=False done_reason=None
    total_duration=None load_duration=None
    prompt_eval_count=None prompt_eval_duration=None
    eval_count=None eval_duration=None
    message=Message ( role='assistant', content=' like', images=None, tool_calls=None ) }
